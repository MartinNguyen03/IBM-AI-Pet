{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from efficientnet_lite import EfficientNetLiteB0\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from IPython.display import clear_output\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetlite0 (Function  (None, 4, 4, 1280)       3413024   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 20480)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 61443     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,474,467\n",
      "Trainable params: 3,432,451\n",
      "Non-trainable params: 42,016\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        # preprocessing.RandomZoom(height_factor=0.2),\n",
    "        preprocessing.RandomRotation(0.15)\n",
    "    ]\n",
    ")\n",
    "\n",
    "backbone = EfficientNetLiteB0(\n",
    "   weights=None, \n",
    "   input_shape=(128,128, 3),\n",
    "   include_top=False\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    backbone,\n",
    "    Flatten(),\n",
    "    Dense(3, activation = 'softmax')  # 2 = num classes\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=3, restore_best_weights=True)\n",
    "model.compile(optimizer=Adam(lr=1e-4),loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "[9.9992740e-01 5.1951648e-14 7.2625087e-05]\n"
     ]
    }
   ],
   "source": [
    "clock = 0\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "dir_save = r\"C:\\Users\\Jerry\\OneDrive\\Documents\\Imperial\\3rd year\\3rd year Group project\\Image\\Train data\\Faces\\%d.jpeg\" # Directory folder to store user facial data\n",
    "\n",
    "while True:\n",
    "    success, frame = video_capture.read()  # read frames from the video\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    if clock % 100 == 0:\n",
    "        clear_output()\n",
    "        frame.resize([128, 128, 3])\n",
    "        frame = frame / 255.\n",
    "        score_raw = model.predict(frame[None])[0]\n",
    "        print(score_raw)\n",
    "        score = tf.math.argmax(score_raw)\n",
    "    if score == 1:\n",
    "        cv2.putText(frame, 'Hi boss!', (50, 50), cv2.LINE_AA, fontScale = 1, color = (255, 0, 0))\n",
    "    elif score == 0:\n",
    "        cv2.putText(frame, 'Anyone there?', (50, 50), cv2.LINE_AA, fontScale = 1, color = (255, 0, 0))\n",
    "\n",
    "    cv2.imshow(\"Test\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"z\"):\n",
    "        for i in range(20):\n",
    "            success, image = video_capture.read()\n",
    "            if not success:\n",
    "                break\n",
    "            cv2.putText(frame, 'Taking pictures...', (50, 50), cv2.LINE_AA, fontScale = 1, color = (255, 0, 0))\n",
    "            cv2.imshow(\"Test\", frame)\n",
    "            cv2.imwrite(dir_save % (81 + i), image)\n",
    "        print('Done.')\n",
    "\n",
    "    elif cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "    clock += 1\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = r'C:\\Users\\Jerry\\OneDrive\\Documents\\Imperial\\3rd year\\3rd year Group project\\Image\\Train data\\Faces' # Directory folder which stores all facial data\n",
    "data = tf.keras.preprocessing.image_dataset_from_directory(dir, image_size=(128, 128), batch_size = 300, label_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in data.take(1):  # Takes the 1st batch; in our case there's only 1 batch\n",
    "    train_X = images.numpy() / 255.\n",
    "    labels = labels.numpy()\n",
    "\n",
    "labels = to_categorical(labels, 3)\n",
    "print(labels[5])\n",
    "plt.imshow(train_X[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 26s 78ms/step - loss: 0.1489 - categorical_accuracy: 0.9704 - val_loss: 0.7665 - val_categorical_accuracy: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fa2864a510>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, labels, epochs=1, batch_size = 1, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_raw = model.predict(train_X[0][None])[0]\n",
    "score = tf.math.argmax(score_raw)\n",
    "plt.imshow(train_X[0])\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
